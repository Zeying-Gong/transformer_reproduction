{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5Rek5EFU/NWDVyyr5MMdz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serika-onoe/transformer_reproduction/blob/main/Transformer_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer 的 Pytorch 复现简易版教程"
      ],
      "metadata": {
        "id": "0bVIW5rC6Axe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "并没有用到大型的数据集，而是手动输入了两对中文→英语的句子，还有每个字的索引也是我手动硬编码上去的，主要是为了降低代码阅读难度，**把重点放到模型实现的部分**"
      ],
      "metadata": {
        "id": "BemmlxBdOTQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 数据预处理"
      ],
      "metadata": {
        "id": "vwl22daO6L8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QvJBOsfM32an"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "\n",
        "device = 'cpu'\n",
        "# device = 'cuda'\n",
        "\n",
        "# transformer epochs\n",
        "epochs = 100\n",
        "# epochs = 1000\n",
        "\n",
        "# 这里我没有用什么大型的数据集，而是手动输入了两对中文→英语的句子\n",
        "# 还有每个字的索引也是我手动硬编码上去的，主要是为了降低代码阅读难度\n",
        "# S: Symbol that shows starting of decoding input\n",
        "# E: Symbol that shows starting of decoding output\n",
        "# P: Symbol that will fill in blank sequence if current batch data size is shorter than time steps\n",
        "\n",
        "# 训练集\n",
        "sentences = [\n",
        "    # 中文和英语的单词个数不要求相同\n",
        "    # enc_input                dec_input           dec_output\n",
        "    ['我 有 一 个 女 朋 友', 'S i have a girl friend . ', 'i have a girl friend . E'],\n",
        "    ['我 有 零 个 好 朋 友', 'S i have zero good friend .', 'i have zero good friend . E']\n",
        "]\n",
        "\n",
        "# 中文和英语的单词要分开建立词库\n",
        "# Padding Should be Zero\n",
        "src_vocab = {\n",
        "            'P': 0, '我': 1, '有': 2, '一': 3, '个': 4, '好': 5, '朋': 6, '友': 7, '零': 8, '女': 9}\n",
        "src_idx2word = {\n",
        "            i: w for i, w in enumerate(src_vocab)}\n",
        "src_vocab_size = len(src_vocab)\n",
        "\n",
        "tgt_vocab = {\n",
        "            'P': 0, 'i': 1, 'have': 2, 'a': 3, 'good': 4, 'friend': 5, 'zero': 6, 'girl': 7, 'S': 8, 'E': 9, '.': 10}\n",
        "idx2word = {\n",
        "            i: w for i, w in enumerate(tgt_vocab)}\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "src_len = 8  # （源句子的长度）enc_input max sequence length\n",
        "tgt_len = 7  # dec_input(=dec_output) max sequence length\n",
        "\n",
        "# Transformer Parameters\n",
        "d_model = 512  # Embedding Size（token embedding和position编码的维度）\n",
        "d_ff = 2048  # FeedForward dimension (两次线性层中的隐藏层 512->2048->512，线性层是用来做特征提取的），当然最后会再接一个projection层\n",
        "d_k = d_v = 64  # dimension of K(=Q), V（Q和K的维度需要相同，这里为了方便让K=V）\n",
        "n_layers = 6  # number of Encoder of Decoder Layer（Block的个数）\n",
        "n_heads = 8  # number of heads in Multi-Head Attention（有几套头）\n",
        "\n",
        "\n",
        "# ==============================================================================================\n",
        "# 数据构建\n",
        "\n",
        "\n",
        "def make_data(sentences):\n",
        "    \"\"\"把单词序列转换为数字序列\"\"\"\n",
        "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
        "    for i in range(len(sentences)):\n",
        "        enc_input = [[src_vocab[n] for n in sentences[i][0].split()]]  # [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\n",
        "        dec_input = [[tgt_vocab[n] for n in sentences[i][1].split()]]  # [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\n",
        "        dec_output = [[tgt_vocab[n] for n in sentences[i][2].split()]]  # [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\n",
        "\n",
        "        enc_inputs.extend(enc_input)\n",
        "        dec_inputs.extend(dec_input)\n",
        "        dec_outputs.extend(dec_output)\n",
        "\n",
        "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\n",
        "\n",
        "\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
        "\n",
        "\n",
        "class MyDataSet(Data.Dataset):\n",
        "    \"\"\"自定义DataLoader\"\"\"\n",
        "\n",
        "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
        "        super(MyDataSet, self).__init__()\n",
        "        self.enc_inputs = enc_inputs\n",
        "        self.dec_inputs = dec_inputs\n",
        "        self.dec_outputs = dec_outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.enc_inputs.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\n",
        "\n",
        "\n",
        "loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n",
        "# 这行代码创建了一个PyTorch数据加载器，用于在训练机器学习模型时加载数据。DataLoader是PyTorch的核心库torch.utils.data中的函数。它的作用是将\n",
        "# 数据集（此处为MyDataSet类的实例，传入的enc_inputs、dec_inputs和dec_outputs作为数据）拆分成小批次以提高加载效率。\n",
        "# 具体参数说明：\n",
        "# batch_size: 2，每批加载的样本数。\n",
        "# shuffle: True，是否打乱数据顺序。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上面都比较简单，下面开始涉及到模型就比较复杂了，因此我会将模型拆分成以下几个部分进行讲解\n",
        "\n",
        "-   Positional Encoding\n",
        "-   Pad Mask（序列本身固定长度，不够长的序列需要填充（pad），也就是'P'）\n",
        "-   Subsequence Mask（Decoder input 不能看到未来时刻单词信息，因此需要 mask）\n",
        "-   ScaledDotProductAttention\n",
        "-   Multi-Head Attention\n",
        "-   FeedForward Layer\n",
        "-   Encoder Layer\n",
        "-   Encoder\n",
        "-   Decoder Layer\n",
        "-   Decoder\n",
        "-   Transformer\n",
        "\n",
        "关于代码中的注释，如果值为 `src_len` 或者 `tgt_len` 的，我一定会写清楚，但是有些函数或者类，Encoder 和 Decoder 都有可能调用，因此就不能确定究竟是 `src_len` 还是 `tgt_len`，对于不确定的，我会记作 `seq_len`"
      ],
      "metadata": {
        "id": "Vl5lV5K_6hXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "CTVTzhf_6xjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \"\"\"\n",
        "        在dropout函数中，参数p是指丢弃元素的概率。它决定了输入张量的元素在丢弃操作中被设置为零的比率。\n",
        "        例如，如果p=0.1，那么10%的元素将在丢弃操作中被设置为零。\n",
        "        \"\"\"\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \"\"\"\n",
        "        通过调用self.register_buffer('pe', pe)，我们将这个张量注册为模型的一个可学习参数，\n",
        "        这意味着这个张量在模型训练过程中不需要更新其梯度，即不需要在损失函数的计算中考虑这个张量的梯度。\n",
        "        \"\"\"\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [seq_len, batch_size, d_model]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "oeQeLxfE4PZJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pad Mask"
      ],
      "metadata": {
        "id": "u6SzjH0360Mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "由于在 Encoder 和 Decoder 中都需要进行 mask 操作，因此就无法确定这个函数的参数中 `seq_len` 的值，如果是在 Encoder 中调用的，`seq_len` 就等于 `src_len`；如果是在 Decoder 中调用的，`seq_len` 就有可能等于 `src_len`，也有可能等于 `tgt_len`（因为 Decoder 有两次 mask）\n",
        "\n",
        "**pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量。**\n",
        "\n",
        "这个函数最核心的一句代码是 `seq_k.data.eq(0)`，这句的作用是返回一个大小和 `seq_k` 一样的 tensor，只不过里面的值只有 True 和 False。如果 `seq_k` 某个位置的值等于 0，那么对应位置就是 True，否则即为 False。举个例子，输入为 `seq_data = [1, 2, 3, 4, 0]`，`seq_data.data.eq(0)` 就会返回 `[False, False, False, False, True]`"
      ],
      "metadata": {
        "id": "pYUDp9Zx69Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    # pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量\n",
        "    \"\"\"这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），例如encoder_inputs (x1,x2,..xm)和encoder_inputs (x1,x2..xm)\n",
        "    encoder和decoder都可能调用这个函数，所以seq_len视情况而定\n",
        "    seq_q: [batch_size, seq_len]\n",
        "    seq_k: [batch_size, seq_len]\n",
        "    seq_len could be src_len or it could be tgt_len\n",
        "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
        "    \"\"\"\n",
        "    batch_size, len_q = seq_q.size()  # 这个seq_q只是用来expand维度的\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    # 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], True is masked\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)"
      ],
      "metadata": {
        "id": "uxEUb78j4V3O"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subsequence Mask"
      ],
      "metadata": {
        "id": "-4tdE1Fa7gDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subsequence Mask 只有 Decoder 会用到，主要作用是屏蔽未来时刻单词的信息。**\n",
        "\n",
        "这段代码实现了获得一个注意力子序列掩码。它使用Numpy函数np.triu生成一个上三角矩阵，并用np.ones初始化这个矩阵。attn_shape变量储存了这个矩阵的形状，它是一个三维数组，分别是batch_size、tgt_len、tgt_len。然后，np.triu将这个矩阵初始化为上三角形，并通过参数k=1使对角线上的元素为0。最后，将这个矩阵转换为PyTorch tensor，并返回该张量。"
      ],
      "metadata": {
        "id": "VQBnTuxj7doJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn_subsequence_mask(seq):\n",
        "    \"\"\"建议打印出来看看是什么样的输出（一目了然）\n",
        "    seq: [batch_size, tgt_len]\n",
        "    \"\"\"\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    # attn_shape: [batch_size, tgt_len, tgt_len]\n",
        "    subsequence_mask = np.triu(np.ones(attn_shape), k=1)  # 生成一个上三角矩阵\n",
        "    subsequence_mask = torch.from_numpy(subsequence_mask).byte()\n",
        "    return subsequence_mask  # [batch_size, tgt_len, tgt_len]"
      ],
      "metadata": {
        "id": "MNTaDFMy4dqf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ScaledDotProductAttention"
      ],
      "metadata": {
        "id": "PLE3ELuV70Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Scaled Dot-Product Attention\" 是一种用于自注意力机制的注意力机制方法，通过将输入矩阵Q、K和V与自身转置进行点积运算，得到关于每一个词的注意力分数。**\n",
        "    \n",
        "使用Q、K、V三个变量作为输入，经过一系列矩阵运算后得到context和attn，其中context是计算出的注意力张量，attn是对应的注意力稀疏矩阵，并返回这两个结果。\n",
        "\n",
        "具体地，在forward函数中，使用Q、K做矩阵乘法得到scores矩阵，scores中的每一个元素都是对应Q中词与K中词的相似程度。\n",
        "\n",
        "下一步，通过使用mask矩阵对scores中的元素进行赋值，将与mask矩阵中值为1的元素相对应的scores元素赋值为-1e9，使其不被softmax计算。\n",
        "\n",
        "最后，使用softmax对scores最后一维（也就是v）做软归一化，得到注意力稀疏矩阵attn。最后，使用attn矩阵对V做矩阵乘法，得到context矩阵，其中每一行对应一个词的向量表示。\n",
        "\n",
        "（matmul函数是矩阵乘法，它返回两个矩阵的点积，即将两个矩阵对应元素相乘并相加。它对应的矩阵乘法操作是：C = A * B，其中C是乘积矩阵，A是左矩阵，B是右矩阵。）\n",
        "    "
      ],
      "metadata": {
        "id": "PBJulLWF8Iyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        Q: [batch_size, n_heads, len_q, d_k]\n",
        "        K: [batch_size, n_heads, len_k, d_k]\n",
        "        V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "        attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
        "        说明：在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同\n",
        "        \"\"\"\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)  # scores : [batch_size, n_heads, len_q, len_k]\n",
        "        # mask矩阵填充scores（用-1e9填充scores中与attn_mask中值为1位置相对应的元素）\n",
        "        scores.masked_fill_(attn_mask, -1e9)  # Fills elements of self tensor with value where mask is True.\n",
        "\n",
        "        attn = nn.Softmax(dim=-1)(scores)  # 对最后一个维度(v)做softmax\n",
        "        # scores : [batch_size, n_heads, len_q, len_k] * V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "        context = torch.matmul(attn, V)  # context: [batch_size, n_heads, len_q, d_v]\n",
        "        # context：[[z1,z2,...],[...]]向量, attn注意力稀疏矩阵（用于可视化的）\n",
        "        return context, attn"
      ],
      "metadata": {
        "id": "iFStSIgs4io6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadAttention"
      ],
      "metadata": {
        "id": "qTT79TUR8b-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "完整代码中一定会有三处地方调用 `MultiHeadAttention()`，Encoder Layer 调用一次，传入的 `input_Q`、`input_K`、`input_V` 全部都是 `enc_inputs`；Decoder Layer 中两次调用，第一次传入的全是 `dec_inputs`，第二次传入的分别是 `dec_outputs`，`enc_outputs`，`enc_outputs`"
      ],
      "metadata": {
        "id": "mArvdqRv812n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"这个Attention类可以实现:\n",
        "    Encoder的Self-Attention\n",
        "    Decoder的Masked Self-Attention\n",
        "    Encoder-Decoder的Attention\n",
        "    输入：seq_len x d_model\n",
        "    输出：seq_len x d_model\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)  # q,k必须维度相同，不然无法做点积\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
        "        # 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model\n",
        "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
        "        \"\"\"\n",
        "        nn.Linear 函数是 PyTorch 模型中的一种全连接层 (fully connected layer) 的实现。\n",
        "        它的作用是对输入数据进行线性变换，即 y = Wx + b，其中 W 是线性变换的系数矩阵，b 是偏移量，x 是输入数据。\n",
        "        torch.nn.Linear(in_features, # 输入的神经元个数\n",
        "           out_features, # 输出神经元个数\n",
        "           bias=True # 是否包含偏置\n",
        "           )\n",
        "        \"\"\"\n",
        "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
        "        \"\"\"\n",
        "        input_Q: [batch_size, len_q, d_model]\n",
        "        input_K: [batch_size, len_k, d_model]\n",
        "        input_V: [batch_size, len_v(=len_k), d_model]\n",
        "        attn_mask: [batch_size, seq_len, seq_len]\n",
        "        \"\"\"\n",
        "        residual, batch_size = input_Q, input_Q.size(0)\n",
        "        # 下面的多头的参数矩阵是放在一起做线性变换的，然后再拆成多个头，这是工程实现的技巧\n",
        "        # B: batch_size, S:seq_len, D: dim\n",
        "        # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, Head, W) -trans-> (B, Head, S, W)\n",
        "        #           线性变换               拆成多头\n",
        "\n",
        "        # Q: [batch_size, n_heads, len_q, d_k]\n",
        "        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
        "        # K: [batch_size, n_heads, len_k, d_k] # K和V的长度一定相同，维度可以不同\n",
        "        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
        "        # V: [batch_size, n_heads, len_v(=len_k), d_v]\n",
        "        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
        "\n",
        "        # 因为是多头，所以mask矩阵要扩充成4维的\n",
        "        # attn_mask: [batch_size, seq_len, seq_len] -> [batch_size, n_heads, seq_len, seq_len]\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
        "\n",
        "        # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\n",
        "        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n",
        "        # 下面将不同头的输出向量拼接在一起\n",
        "        # context: [batch_size, n_heads, len_q, d_v] -> [batch_size, len_q, n_heads * d_v]\n",
        "        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v)\n",
        "\n",
        "        # 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model\n",
        "        output = self.fc(context)  # [batch_size, len_q, d_model]\n",
        "        return nn.LayerNorm(d_model).to(device)(output + residual), attn"
      ],
      "metadata": {
        "id": "75KFTBtZ4jSL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FeedForward Layer"
      ],
      "metadata": {
        "id": "59pLclCH84nJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**这段代码非常简单，就是做两次线性变换，残差连接后再跟一个 Layer Norm。用于实现Transformer模型中的前馈网络。**\n",
        "\n",
        "该网络由两个全连接层（nn.Linear）和一个 ReLU 激活函数（nn.ReLU）组成。第一个全连接层将输入从 d_model 维度转换到 d_ff 维度，第二个全连接层将输入从 d_ff 维度转换回 d_model 维度。"
      ],
      "metadata": {
        "id": "IWM8iMCR89_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model, bias=False)\n",
        "        )\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        residual = inputs\n",
        "        output = self.fc(inputs)\n",
        "        return nn.LayerNorm(d_model).to(device)(output + residual)  # [batch_size, seq_len, d_model]"
      ],
      "metadata": {
        "id": "4DO7MI024nX8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Layer & Encoder"
      ],
      "metadata": {
        "id": "quUrFh7O9H66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        \"\"\"\n",
        "        enc_inputs: [batch_size, src_len, d_model]\n",
        "        enc_self_attn_mask: [batch_size, src_len, src_len]  mask矩阵(pad mask or sequence mask)\n",
        "        \"\"\"\n",
        "        # enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]\n",
        "        # 第一个enc_inputs * W_Q = Q\n",
        "        # 第二个enc_inputs * W_K = K\n",
        "        # 第三个enc_inputs * W_V = V\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,\n",
        "                                               enc_self_attn_mask)  # enc_inputs to same Q,K,V（未线性变换前）\n",
        "        enc_outputs = self.pos_ffn(enc_outputs)\n",
        "        # enc_outputs: [batch_size, src_len, d_model]\n",
        "        return enc_outputs, attn"
      ],
      "metadata": {
        "id": "Qp4g8OB04qfD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nn.ModuleList()` 列表里面存了 `n_layers` 个 Encoder Layer。由于我们控制好了 Encoder Layer 的输入和输出维度相同，所以可以直接用个 for 循环以嵌套的方式，将上一次 Encoder Layer 的输出作为下一次 Encoder Layer 的输入。\n",
        "\n",
        "**将`n_layers`个（本文为6个）EncoderLayer组件逐个拼起来，就是一个完整的Encoder。**"
      ],
      "metadata": {
        "id": "wIZmEuRN9dIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.src_emb = nn.Embedding(src_vocab_size, d_model)  # token Embedding\n",
        "        \"\"\"\n",
        "        Embedding解释\n",
        "        例如：如果你有一个词语表(vocabulary)，其中包含了3个词语：\"dog\", \"cat\", \"bird\"。并且你指定了src_vocab_size = 3\n",
        "        和d_model = 5，那么这个Embedding层就可以将每一个词语表示成一个5维的实数向量，比如：\"dog\"\n",
        "        可以表示为[0.1, 0.2, 0.3, 0.4, 0.5]。\n",
        "        \"\"\"\n",
        "        self.pos_emb = PositionalEncoding(d_model)  # Transformer中位置编码时固定的，不需要学习\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, enc_inputs):\n",
        "        \"\"\"\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        \"\"\"\n",
        "        enc_outputs = self.src_emb(enc_inputs)  # [batch_size, src_len, d_model]\n",
        "        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1)  # [batch_size, src_len, d_model]\n",
        "        # Encoder输入序列的pad mask矩阵\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  # [batch_size, src_len, src_len]\n",
        "        enc_self_attns = []  # 在计算中不需要用到，它主要用来保存你接下来返回的attention的值（这个主要是为了你画热力图等，用来看各个词之间的关系\n",
        "        for layer in self.layers:  # for循环访问nn.ModuleList对象\n",
        "            # 上一个block的输出enc_outputs作为当前block的输入\n",
        "            # enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]\n",
        "            enc_outputs, enc_self_attn = layer(enc_outputs,\n",
        "                                               enc_self_attn_mask)  # 传入的enc_outputs其实是input，传入mask矩阵是因为你要做self attention\n",
        "            enc_self_attns.append(enc_self_attn)  # 这个只是为了可视化\n",
        "        return enc_outputs, enc_self_attns"
      ],
      "metadata": {
        "id": "THcMVFeX4sqn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Layer & Decoder"
      ],
      "metadata": {
        "id": "SnbPJoB8-S9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 Decoder Layer 中会调用两次 `MultiHeadAttention`，第一次是计算 Decoder Input 的 self-attention，得到输出 `dec_outputs`。然后将 `dec_outputs` 作为生成 Q 的元素，`enc_outputs` 作为生成 K 和 V 的元素，再调用一次 `MultiHeadAttention`，得到的是 Encoder 和 Decoder Layer 之间的 context vector。最后将 `dec_outptus` 做一次维度变换，然后返回。\n",
        "\n",
        "**将`n_layers`个（本文为6个）DecoderLayer组件逐个拼起来，就是一个完整的Decoder。**"
      ],
      "metadata": {
        "id": "_KHMSwaF-au0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.dec_self_attn = MultiHeadAttention()\n",
        "        self.dec_enc_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \"\"\"\n",
        "        dec_inputs: [batch_size, tgt_len, d_model]\n",
        "        enc_outputs: [batch_size, src_len, d_model]\n",
        "        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\n",
        "        dec_enc_attn_mask: [batch_size, tgt_len, src_len]\n",
        "        \"\"\"\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\n",
        "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs,\n",
        "                                                        dec_self_attn_mask)  # 这里的Q,K,V全是Decoder自己的输入\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
        "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs,\n",
        "                                                      dec_enc_attn_mask)  # Attention层的Q(来自decoder) 和 K,V(来自encoder)\n",
        "        dec_outputs = self.pos_ffn(dec_outputs)  # [batch_size, tgt_len, d_model]\n",
        "        return dec_outputs, dec_self_attn, dec_enc_attn  # dec_self_attn, dec_enc_attn这两个是为了可视化的"
      ],
      "metadata": {
        "id": "MBIgwWn44vok"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder 中不仅要把 \"pad\"mask 掉，还要 mask 未来时刻的信息，因此就有了下面这三行代码，其中 `torch.gt(a, value)` 的意思是，将 a 中各个位置上的元素和 value 比较，若大于 value，则该位置取 1，否则取 0"
      ],
      "metadata": {
        "id": "5LJaq5Iy-fPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)  # Decoder输入的embed词表\n",
        "        self.pos_emb = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])  # Decoder的blocks\n",
        "\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
        "        \"\"\"\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        enc_outputs: [batch_size, src_len, d_model]   # 用在Encoder-Decoder Attention层\n",
        "        \"\"\"\n",
        "        dec_outputs = self.tgt_emb(dec_inputs)  # [batch_size, tgt_len, d_model]\n",
        "        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).to(\n",
        "            device)  # [batch_size, tgt_len, d_model]\n",
        "        # Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的，实际应用中都是有pad填充的）\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(device)  # [batch_size, tgt_len, tgt_len]\n",
        "        # Masked Self_Attention：当前时刻是看不到未来的信息的\n",
        "        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(\n",
        "            device)  # [batch_size, tgt_len, tgt_len]\n",
        "\n",
        "        # Decoder中把两种mask矩阵相加（既屏蔽了pad的信息，也屏蔽了未来时刻的信息）\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),\n",
        "                                      0).to(device)  # [batch_size, tgt_len, tgt_len]; torch.gt比较两个矩阵的元素，大于则返回1，否则返回0\n",
        "\n",
        "        # 这个mask主要用于encoder-decoder attention层\n",
        "        # get_attn_pad_mask主要是enc_inputs的pad mask矩阵(因为enc是处理K,V的，求Attention时是用v1,v2,..vm去加权的，要把pad对应的v_i的相关系数设为0，这样注意力就不会关注pad向量)\n",
        "        #                       dec_inputs只是提供expand的size的\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)  # [batc_size, tgt_len, src_len]\n",
        "\n",
        "        dec_self_attns, dec_enc_attns = [], []\n",
        "        for layer in self.layers:\n",
        "            # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\n",
        "            # Decoder的Block是上一个Block的输出dec_outputs（变化）和Encoder网络的输出enc_outputs（固定）\n",
        "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask,\n",
        "                                                             dec_enc_attn_mask)\n",
        "            dec_self_attns.append(dec_self_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model]\n",
        "        return dec_outputs, dec_self_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "e7ClC1Em4yfA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "cq0RB-uw-qsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**这段代码实现了Transformer的前向传播。**它接受两个序列：enc_inputs和dec_inputs。\n",
        "\n",
        "在输入经过Encoder网络和Decoder网络处理后，得到的输出分别是enc_outputs和dec_outputs。最后再经过一个投影层，将dec_outputs映射成dec_logits，表示每个单词的词概率分布。返回 dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns。\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "qLmWKVrA-tAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        # Transformer类继承自PyTorch的nn.Module类。\n",
        "        # 通过在Transformer的构造函数中调用super(Transformer, self).init()，\n",
        "        # 可以调用nn.Module的构造函数，以便初始化nn.Module的一些内部状态，以及设置Transformer类对象的一些公共属性。\n",
        "        self.encoder = Encoder().to(device)\n",
        "        self.decoder = Decoder().to(device)\n",
        "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).to(device)\n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        \"\"\"Transformers的输入：两个序列\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        \"\"\"\n",
        "        # tensor to store decoder outputs\n",
        "        # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        # enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\n",
        "        # 经过Encoder网络后，得到的输出还是[batch_size, src_len, d_model]\n",
        "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\n",
        "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
        "        # dec_outputs: [batch_size, tgt_len, d_model] -> dec_logits: [batch_size, tgt_len, tgt_vocab_size]\n",
        "        dec_logits = self.projection(dec_outputs)\n",
        "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "H88P7zkc41Hd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**view函数说明**\n",
        "\n",
        "view 方法是 PyTorch 中 tensor 的一种 reshape 操作。它将一个 tensor 的 shape 变成给定的形状。\n",
        "\n",
        "在这段代码中， dec_logits.view(-1, dec_logits.size(-1)) 表示将 dec_logits tensor 从原来的 shape 变成了一个新的 shape，其中第一维是 -1，这意味着该维的长度是自动计算的（其他维度的长度已经确定了），第二维是 dec_logits.size(-1)，这是一个数字，代表 dec_logits tensor 的最后一维的长度。"
      ],
      "metadata": {
        "id": "kz5d6DQ__OSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**forward函数说明：**\n",
        "\n",
        "通过进行一次前向传播的操作（比如 output = model(input)）时，PyTorch 内部会对模型中每一个模块（包括 Encoder 和 EncoderLayer）中的 forward 函数进行调用，以计算输出结果。\n",
        "\n",
        "如果不手动定义 forward 函数，那么模型将不会被调用。因此，forward 函数是必须被定义的，用于计算模型的前向传播过程。"
      ],
      "metadata": {
        "id": "NxjDJaU7_Kew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型调用 & 损失函数 & 优化器"
      ],
      "metadata": {
        "id": "jejdRKjP_dAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码调用了Transformer模型，并设置了交叉熵损失函数（将ignore_index参数设置为0），优化器使用随机梯度下降（SGD）算法。（优化器将使用model.parameters()作为参数进行优化，学习率为1e-3，动量为0.99）。\n",
        "\n",
        "ignore_index参数被设置为0，这样损失计算将忽略任何索引为0的输入，这通常是为NLP模型中的padding token保留的。\n",
        "\n",
        "使用Adam算法对于较小的数据量效果很差"
      ],
      "metadata": {
        "id": "CF5kFd_c_l22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer().to(device)\n",
        "# 这里的损失函数里面设置了一个参数 ignore_index=0，因为 \"pad\" 这个单词的索引为 0，这样设置以后，就不会计算 \"pad\" 的损失（因为本来 \"pad\" 也没有意义，不需要计算）\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-9) # 用adam的话效果不好"
      ],
      "metadata": {
        "id": "k4Y16nve5JsI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练"
      ],
      "metadata": {
        "id": "evtTI23gACVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**最后三行代码是在进行一次反向传播迭代的操作。**\n",
        "\n",
        "分三步执行：\n",
        "optimizer.zero_grad()：对梯度进行初始化，因为pytorch的梯度是累加的，所以每次计算前需要把梯度归零。\n",
        "loss.backward()：计算当前损失函数的梯度，并且完成反向传播。\n",
        "optimizer.step()：执行优化器的更新操作，根据梯度对模型参数进行更新。\n",
        "\n",
        "总的来说，这三步代码是完成一次机器学习模型的参数优化的核心过程。"
      ],
      "metadata": {
        "id": "-8x9sa76_8Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    for enc_inputs, dec_inputs, dec_outputs in loader:\n",
        "        \"\"\"\n",
        "        enc_inputs: [batch_size, src_len]\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        dec_outputs: [batch_size, tgt_len]\n",
        "        \"\"\"\n",
        "        enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)\n",
        "        # outputs: [batch_size * tgt_len, tgt_vocab_size]\n",
        "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
        "        loss = criterion(outputs, dec_outputs.view(-1))  # dec_outputs.view(-1):[batch_size * tgt_len * tgt_vocab_size]\n",
        "        print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJb4CyFD5Nt8",
        "outputId": "19d888e8-ff8b-4594-8d21-435b7ac71816"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss = 2.752963\n",
            "Epoch: 0002 loss = 2.625792\n",
            "Epoch: 0003 loss = 2.508290\n",
            "Epoch: 0004 loss = 2.260287\n",
            "Epoch: 0005 loss = 2.026300\n",
            "Epoch: 0006 loss = 1.715278\n",
            "Epoch: 0007 loss = 1.559423\n",
            "Epoch: 0008 loss = 1.339594\n",
            "Epoch: 0009 loss = 1.082600\n",
            "Epoch: 0010 loss = 0.945365\n",
            "Epoch: 0011 loss = 0.758650\n",
            "Epoch: 0012 loss = 0.581669\n",
            "Epoch: 0013 loss = 0.461126\n",
            "Epoch: 0014 loss = 0.340352\n",
            "Epoch: 0015 loss = 0.252243\n",
            "Epoch: 0016 loss = 0.190971\n",
            "Epoch: 0017 loss = 0.161172\n",
            "Epoch: 0018 loss = 0.130554\n",
            "Epoch: 0019 loss = 0.101027\n",
            "Epoch: 0020 loss = 0.093038\n",
            "Epoch: 0021 loss = 0.079294\n",
            "Epoch: 0022 loss = 0.070799\n",
            "Epoch: 0023 loss = 0.062795\n",
            "Epoch: 0024 loss = 0.044259\n",
            "Epoch: 0025 loss = 0.056274\n",
            "Epoch: 0026 loss = 0.033928\n",
            "Epoch: 0027 loss = 0.037328\n",
            "Epoch: 0028 loss = 0.035663\n",
            "Epoch: 0029 loss = 0.032550\n",
            "Epoch: 0030 loss = 0.029425\n",
            "Epoch: 0031 loss = 0.028057\n",
            "Epoch: 0032 loss = 0.024588\n",
            "Epoch: 0033 loss = 0.019545\n",
            "Epoch: 0034 loss = 0.025953\n",
            "Epoch: 0035 loss = 0.018335\n",
            "Epoch: 0036 loss = 0.028104\n",
            "Epoch: 0037 loss = 0.015952\n",
            "Epoch: 0038 loss = 0.014356\n",
            "Epoch: 0039 loss = 0.015536\n",
            "Epoch: 0040 loss = 0.013210\n",
            "Epoch: 0041 loss = 0.015791\n",
            "Epoch: 0042 loss = 0.013085\n",
            "Epoch: 0043 loss = 0.011149\n",
            "Epoch: 0044 loss = 0.009110\n",
            "Epoch: 0045 loss = 0.007416\n",
            "Epoch: 0046 loss = 0.005960\n",
            "Epoch: 0047 loss = 0.006156\n",
            "Epoch: 0048 loss = 0.004907\n",
            "Epoch: 0049 loss = 0.004867\n",
            "Epoch: 0050 loss = 0.005042\n",
            "Epoch: 0051 loss = 0.005796\n",
            "Epoch: 0052 loss = 0.005398\n",
            "Epoch: 0053 loss = 0.004669\n",
            "Epoch: 0054 loss = 0.004401\n",
            "Epoch: 0055 loss = 0.003372\n",
            "Epoch: 0056 loss = 0.002630\n",
            "Epoch: 0057 loss = 0.002565\n",
            "Epoch: 0058 loss = 0.002309\n",
            "Epoch: 0059 loss = 0.003040\n",
            "Epoch: 0060 loss = 0.002470\n",
            "Epoch: 0061 loss = 0.002096\n",
            "Epoch: 0062 loss = 0.002189\n",
            "Epoch: 0063 loss = 0.002061\n",
            "Epoch: 0064 loss = 0.001174\n",
            "Epoch: 0065 loss = 0.001599\n",
            "Epoch: 0066 loss = 0.001527\n",
            "Epoch: 0067 loss = 0.001685\n",
            "Epoch: 0068 loss = 0.001565\n",
            "Epoch: 0069 loss = 0.001718\n",
            "Epoch: 0070 loss = 0.001291\n",
            "Epoch: 0071 loss = 0.001259\n",
            "Epoch: 0072 loss = 0.001222\n",
            "Epoch: 0073 loss = 0.001179\n",
            "Epoch: 0074 loss = 0.000965\n",
            "Epoch: 0075 loss = 0.001888\n",
            "Epoch: 0076 loss = 0.001052\n",
            "Epoch: 0077 loss = 0.000888\n",
            "Epoch: 0078 loss = 0.001349\n",
            "Epoch: 0079 loss = 0.000916\n",
            "Epoch: 0080 loss = 0.001315\n",
            "Epoch: 0081 loss = 0.001191\n",
            "Epoch: 0082 loss = 0.001341\n",
            "Epoch: 0083 loss = 0.001674\n",
            "Epoch: 0084 loss = 0.001122\n",
            "Epoch: 0085 loss = 0.001133\n",
            "Epoch: 0086 loss = 0.000839\n",
            "Epoch: 0087 loss = 0.001059\n",
            "Epoch: 0088 loss = 0.001204\n",
            "Epoch: 0089 loss = 0.001092\n",
            "Epoch: 0090 loss = 0.000943\n",
            "Epoch: 0091 loss = 0.000699\n",
            "Epoch: 0092 loss = 0.001015\n",
            "Epoch: 0093 loss = 0.000730\n",
            "Epoch: 0094 loss = 0.000795\n",
            "Epoch: 0095 loss = 0.000926\n",
            "Epoch: 0096 loss = 0.000948\n",
            "Epoch: 0097 loss = 0.000945\n",
            "Epoch: 0098 loss = 0.000730\n",
            "Epoch: 0099 loss = 0.000747\n",
            "Epoch: 0100 loss = 0.000749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 测试"
      ],
      "metadata": {
        "id": "vxNgOI1KALsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段代码是一个贪心解码器(greedy decoder)的实现，其作用是在给定编码输入(enc_input)和起始符号(start_symbol)的情况下，根据给定的模型(model)预测出目标序列(greedy_dec_predict)。\n",
        "\n",
        "首先，编码器(encoder)对编码输入(enc_input)进行处理，生成编码输出(enc_outputs)和注意力权值(enc_self_attns)。\n",
        "\n",
        "然后初始化解码器(decoder)的输入(dec_input)为一个空的tensor。\n",
        "\n",
        "接着，在没有到达终止符的情况下，不断执行以下步骤：\n",
        "1. 将解码器的输入(dec_input)拼接上当前的符号(next_symbol)。\n",
        "2. 解码器(decoder)对拼接后的输入(dec_input)、编码输入(enc_input)和编码输出(enc_outputs)进行处理，生成解码输出(dec_outputs)。\n",
        "3. 投影层(projection)将解码输出(dec_outputs)映射到词表上，生成预测概率分布(projected)。\n",
        "4. 根据预测概率分布(projected)，选择概率最大的下一个词，并将其作为下一个符号(next_symbol)。\n",
        "5. 如果下一个符号是终止符，终止循环。\n",
        "\n",
        "最后，返回除开初始符号以外的预测的目标序列(greedy_dec_predict)。"
      ],
      "metadata": {
        "id": "KIEz7qtwAUaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decoder(model, enc_input, start_symbol):\n",
        "    \"\"\"贪心编码\n",
        "    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don't know the\n",
        "    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.\n",
        "    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding\n",
        "    :param model: Transformer Model\n",
        "    :param enc_input: The encoder input\n",
        "    :param start_symbol: The start symbol. In this example it is 'S' which corresponds to index 8\n",
        "    :return: The target input\n",
        "    \"\"\"\n",
        "    enc_outputs, enc_self_attns = model.encoder(enc_input)\n",
        "    dec_input = torch.zeros(1, 0).type_as(enc_input.data)  # 初始化一个空的tensor: tensor([], size=(1, 0), dtype=torch.int64)\n",
        "    terminal = False\n",
        "    next_symbol = start_symbol\n",
        "    while not terminal:\n",
        "        # 预测阶段：dec_input序列会一点点变长（每次添加一个新预测出来的单词）\n",
        "        dec_input = torch.cat([dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],\n",
        "                              -1)\n",
        "        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)\n",
        "        projected = model.projection(dec_outputs)\n",
        "        prob = projected.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
        "        # 增量更新（我们希望重复单词预测结果是一样的）\n",
        "        # 我们在预测时会选择性忽略重复的预测的词，只摘取最新预测的单词拼接到输入序列中\n",
        "        next_word = prob.data[-1]  # 拿出当前预测的单词(数字)。我们用x'_t对应的输出z_t去预测下一个单词的概率，不用z_1,z_2..z_{t-1}\n",
        "        next_symbol = next_word\n",
        "        if next_symbol == tgt_vocab[\"E\"]:\n",
        "            terminal = True\n",
        "        # print(next_word)\n",
        "\n",
        "    # greedy_dec_predict = torch.cat(\n",
        "    #     [dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],\n",
        "    #     -1)\n",
        "    greedy_dec_predict = dec_input[:, 1:]\n",
        "    return greedy_dec_predict"
      ],
      "metadata": {
        "id": "BTXV86yD5Rmq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "这段代码实现了一个简单的预测（因为数据量较小，因此测试集选用训练集中的一句）\n",
        "\n",
        "- 测试集（希望transformer能达到的效果）\n",
        "- 输入：\"我 有 一 个 女 朋 友\"\n",
        "- 输出：\"i have a girl friend\"\n",
        "\n",
        "过程如下：\n",
        "\n",
        "1. 定义了一个句子列表sentences，其中包含一个中文句子和对应的空的英文句子。\n",
        "2. 使用make_data函数处理句子列表，获得编码句子、解码句子的输入和输出。\n",
        "3. 创建一个数据加载器test_loader，用于加载处理后的句子数据。\n",
        "4. 使用next函数从数据加载器中读取一个批次的数据，并将其分别赋给编码句子的输入。\n",
        "5. 对于每个编码句子，调用greedy_decoder函数，通过训练好的Transformer模型，将其翻译成英文句子。\n",
        "6. 最后，输出编码句子和对应的解码句子，以及它们的中英文词语对应关系。"
      ],
      "metadata": {
        "id": "kBqHL5fsA3NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================\n",
        "# 预测阶段\n",
        "# 测试集\n",
        "sentences = [\n",
        "    # enc_input                dec_input           dec_output\n",
        "    ['我 有 一 个 女 朋 友', '', '']\n",
        "]\n",
        "\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
        "test_loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n",
        "enc_inputs, _, _ = next(iter(test_loader))\n",
        "print()\n",
        "print(\"=\"*30)\n",
        "print(\"利用训练好的Transformer模型将中文句子'我 有 一 个 女 朋 友' 翻译成英文句子: \")\n",
        "for i in range(len(enc_inputs)):\n",
        "    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(1, -1).to(device), start_symbol=tgt_vocab[\"S\"])\n",
        "    print(enc_inputs[i], '->', greedy_dec_predict.squeeze())\n",
        "    print([src_idx2word[t.item()] for t in enc_inputs[i]], '->',\n",
        "          [idx2word[n.item()] for n in greedy_dec_predict.squeeze()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z08CqcnNA2PN",
        "outputId": "c6456dbc-809f-43a1-c5b0-27e5a1ca7e40"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "利用训练好的Transformer模型将中文句子'我 有 一 个 女 朋 友' 翻译成英文句子: \n",
            "tensor([1, 2, 3, 4, 9, 6, 7]) -> tensor([ 1,  2,  3,  7,  5, 10])\n",
            "['我', '有', '一', '个', '女', '朋', '友'] -> ['i', 'have', 'a', 'girl', 'friend', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**next 函数说明：**\n",
        "\n",
        "\"next\" 函数用于返回迭代器的下一个项目，即从迭代器中获取下一个数据项。在这段代码中，使用 next(iter(test_loader)) 获取第一个批次的数据，赋值给 enc_inputs, _ , _ 三个变量。"
      ],
      "metadata": {
        "id": "rzcElxgPA9tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**因为数据量较小，如果测试集选用新的句子，那么结果就不尽人意**"
      ],
      "metadata": {
        "id": "w8Rk-ICXNaPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================\n",
        "# 预测阶段\n",
        "# 测试集\n",
        "sentences = [\n",
        "    # enc_input                dec_input           dec_output\n",
        "    ['我 有 一 个 好 朋 友', '', '']\n",
        "]\n",
        "\n",
        "enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\n",
        "test_loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), 2, True)\n",
        "enc_inputs, _, _ = next(iter(test_loader))\n",
        "print()\n",
        "print(\"=\"*30)\n",
        "print(\"利用训练好的Transformer模型将中文句子'我 有 一 个 好 朋 友' 翻译成英文句子: \")\n",
        "for i in range(len(enc_inputs)):\n",
        "    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(1, -1).to(device), start_symbol=tgt_vocab[\"S\"])\n",
        "    print(enc_inputs[i], '->', greedy_dec_predict.squeeze())\n",
        "    print([src_idx2word[t.item()] for t in enc_inputs[i]], '->',\n",
        "          [idx2word[n.item()] for n in greedy_dec_predict.squeeze()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d723dd1a-fa73-48c8-9f7b-7cda6a2a6884",
        "id": "17WYNEBSNTEm"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "利用训练好的Transformer模型将中文句子'我 有 一 个 好 朋 友' 翻译成英文句子: \n",
            "tensor([1, 2, 3, 4, 5, 6, 7]) -> tensor([ 1,  2,  3,  7,  5, 10])\n",
            "['我', '有', '一', '个', '好', '朋', '友'] -> ['i', 'have', 'a', 'girl', 'friend', '.']\n"
          ]
        }
      ]
    }
  ]
}